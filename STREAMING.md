# Streaming Support

The gopenrouter client provides comprehensive streaming support for both completion and chat completion endpoints, enabling real-time response generation from AI models.

> ðŸ“– **Main Documentation**: For general library usage, installation, and basic examples, see the [README.md](README.md).

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
  - [Chat Completion Streaming](#chat-completion-streaming)
  - [Completion Streaming](#completion-streaming)
- [API Reference](#api-reference)
  - [Streaming Methods](#streaming-methods)
  - [Stream Readers](#stream-readers)
  - [Response Types](#response-types)
- [Features](#features)
- [Best Practices](#best-practices)
- [Advanced Usage](#advanced-usage)
- [Error Scenarios](#error-scenarios)
- [Performance Considerations](#performance-considerations)
- [Limitations](#limitations)
- [Migration from Non-Streaming](#migration-from-non-streaming)

## Overview

Streaming allows you to receive partial responses as they are generated by the AI model, rather than waiting for the complete response. This is particularly useful for:

- Building interactive chat interfaces
- Displaying responses as they are generated
- Reducing perceived latency in applications
- Implementing real-time AI-powered features

## Quick Start

> ðŸ’¡ **Prerequisites**: Make sure you have [installed the library](README.md#installation) and understand [basic client setup](README.md#creating-a-client) before using streaming features.

### Chat Completion Streaming

```go
package main

import (
    "context"
    "fmt"
    "io"
    "log"
    
    "github.com/bkovacki/gopenrouter"
)

func main() {
    client := gopenrouter.New("your-api-key")
    
    messages := []gopenrouter.ChatMessage{
        {Role: "user", Content: "Tell me a story"},
    }
    
    request := gopenrouter.NewChatCompletionRequestBuilder(
        "openai/gpt-3.5-turbo", 
        messages,
    ).Build()
    
    stream, err := client.ChatCompletionStream(context.Background(), *request)
    if err != nil {
        log.Fatal(err)
    }
    defer stream.Close()
    
    for {
        chunk, err := stream.Recv()
        if err == io.EOF {
            break
        }
        if err != nil {
            log.Fatal(err)
        }
        
        for _, choice := range chunk.Choices {
            if choice.Delta.Content != nil {
                fmt.Print(*choice.Delta.Content)
            }
        }
    }
}
```

### Completion Streaming

```go
request := gopenrouter.NewCompletionRequestBuilder(
    "openai/gpt-3.5-turbo-instruct",
    "Write a poem about Go programming:",
).Build()

stream, err := client.CompletionStream(context.Background(), request)
if err != nil {
    log.Fatal(err)
}
defer stream.Close()

for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    
    for _, choice := range chunk.Choices {
        if choice.Text != "" {
            fmt.Print(choice.Text)
        }
    }
}
```

## API Reference

### Streaming Methods

#### `ChatCompletionStream(ctx context.Context, request ChatCompletionRequest) (*ChatCompletionStreamReader, error)`

Creates a streaming chat completion request. The method automatically enables streaming and returns a reader for processing chunks.

#### `CompletionStream(ctx context.Context, request CompletionRequest) (*CompletionStreamReader, error)`

Creates a streaming completion request. The method automatically enables streaming and returns a reader for processing chunks.

### Stream Readers

Both streaming methods return readers that implement:

```go
type StreamReader[T any] interface {
    Recv() (T, error)  // Read next chunk
    Close() error      // Close stream and cleanup
}
```

#### ChatCompletionStreamReader

- **Recv()**: Returns `ChatCompletionStreamResponse` chunks
- **Close()**: Closes the underlying HTTP connection

#### CompletionStreamReader

- **Recv()**: Returns `CompletionStreamResponse` chunks  
- **Close()**: Closes the underlying HTTP connection

### Response Types

#### ChatCompletionStreamResponse

```go
type ChatCompletionStreamResponse struct {
    ID      string                `json:"id"`
    Object  string                `json:"object"`
    Created int64                 `json:"created"`
    Model   string                `json:"model"`
    Choices []ChatStreamingChoice `json:"choices"`
    Usage   *Usage                `json:"usage,omitempty"`
}

type ChatStreamingChoice struct {
    Index        int       `json:"index"`
    Delta        ChatDelta `json:"delta"`
    FinishReason *string   `json:"finish_reason"`
}

type ChatDelta struct {
    Role    *string `json:"role,omitempty"`
    Content *string `json:"content,omitempty"`
}
```

#### CompletionStreamResponse

```go
type CompletionStreamResponse struct {
    ID               string            `json:"id"`
    Provider         string            `json:"provider"`
    Model            string            `json:"model"`
    Object           string            `json:"object"`
    Created          int64             `json:"created"`
    Choices          []StreamingChoice `json:"choices"`
    SystemFingerprint *string          `json:"system_fingerprint,omitempty"`
    Usage            *Usage            `json:"usage,omitempty"`
}

type StreamingChoice struct {
    Index              int     `json:"index"`
    Text               string  `json:"text"`
    FinishReason       *string `json:"finish_reason"`
    NativeFinishReason *string `json:"native_finish_reason"`
    Logprobs           *string `json:"logprobs"`
}
```

## Features

### Automatic Stream Management

- Stream parameter is automatically set to `true`
- Proper HTTP headers are set (`Accept: text/event-stream`)
- Server-Sent Events (SSE) parsing is handled internally

### Error Handling

- Network errors are propagated through `Recv()`
- Malformed chunks are skipped automatically
- `io.EOF` indicates successful stream completion
- Context cancellation is supported

### Resource Management

- Always call `defer stream.Close()` to cleanup resources
- Streams automatically handle connection timeouts
- Early termination is supported via context cancellation

## Best Practices

### 1. Always Close Streams

```go
stream, err := client.ChatCompletionStream(ctx, request)
if err != nil {
    return err
}
defer stream.Close() // Always close!
```

### 2. Handle EOF Properly

```go
for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break // Normal completion
    }
    if err != nil {
        return err // Handle error
    }
    // Process chunk
}
```

### 3. Buffer Content

```go
var fullResponse strings.Builder

for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        return err
    }
    
    for _, choice := range chunk.Choices {
        if choice.Delta.Content != nil {
            fullResponse.WriteString(*choice.Delta.Content)
        }
    }
}

completeMessage := fullResponse.String()
```

### 4. Use Context for Timeouts

```go
ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
defer cancel()

stream, err := client.ChatCompletionStream(ctx, request)
```

### 5. Monitor Finish Reasons

```go
for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        return err
    }
    
    for _, choice := range chunk.Choices {
        if choice.FinishReason != nil {
            switch *choice.FinishReason {
            case "stop":
                // Normal completion
            case "length":
                // Hit token limit
            case "content_filter":
                // Content filtered
            }
        }
    }
}
```

## Advanced Usage

> ðŸ”— **Related**: See [Advanced Provider Routing](README.md#advanced-provider-routing) in the main documentation for more provider configuration options.

### Model Fallback with Streaming

```go
request := gopenrouter.NewChatCompletionRequestBuilder(
    "openai/gpt-4", 
    messages,
).WithModels([]string{
    "openai/gpt-4",
    "openai/gpt-3.5-turbo",
    "anthropic/claude-3-haiku",
}).Build()

stream, err := client.ChatCompletionStream(ctx, *request)
// Will automatically fall back to available models
```

### Custom Provider Options

```go
provider := gopenrouter.NewProviderOptionsBuilder().
    WithAllowFallbacks(true).
    WithMaxRequestPrice(0.01).
    Build()

request := gopenrouter.NewChatCompletionRequestBuilder(model, messages).
    WithProvider(provider).
    Build()

stream, err := client.ChatCompletionStream(ctx, *request)
```

### Structured Output with Streaming

```go
request := gopenrouter.NewChatCompletionRequestBuilder(model, messages).
    WithTemperature(0.7).
    WithMaxTokens(500).
    Build()

stream, err := client.ChatCompletionStream(ctx, *request)
```

## Error Scenarios

### Stream Interruption

If a stream is interrupted (network issues, server errors), `Recv()` will return an error:

```go
chunk, err := stream.Recv()
if err != nil && err != io.EOF {
    // Handle error - could be network, timeout, or server error
    log.Printf("Stream error: %v", err)
}
```

### Context Cancellation

Streams respect context cancellation:

```go
ctx, cancel := context.WithCancel(context.Background())

go func() {
    time.Sleep(5 * time.Second)
    cancel() // Cancel after 5 seconds
}()

stream, err := client.ChatCompletionStream(ctx, request)
// Stream will be cancelled when context is cancelled
```

### Invalid Responses

The client automatically skips malformed chunks and continues processing.
Only valid chunks are returned from `Recv()`.

## Performance Considerations

- Streaming reduces time-to-first-token significantly
- Network bandwidth usage is similar to non-streaming
- Memory usage is lower as responses are processed incrementally
- CPU usage is slightly higher due to incremental parsing

## Limitations

- Some models may not support streaming
- Provider-specific features may vary
- Error details in streams may be limited compared to non-streaming responses

## Migration from Non-Streaming

> ðŸ“š **Reference**: For more examples of non-streaming usage, see the [main README examples](README.md#examples).

To convert existing non-streaming code:

```go
// Before: Non-streaming
response, err := client.ChatCompletion(ctx, request)
if err != nil {
    return err
}
content := response.Choices[0].Message.Content

// After: Streaming
stream, err := client.ChatCompletionStream(ctx, request)
if err != nil {
    return err
}
defer stream.Close()

var content strings.Builder
for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        return err
    }
    
    for _, choice := range chunk.Choices {
        if choice.Delta.Content != nil {
            content.WriteString(*choice.Delta.Content)
        }
    }
}
fullContent := content.String()
```
